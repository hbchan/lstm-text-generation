{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMe6TD30wSgtI+H9ccjGNrX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hbchan/lstm-text-generation/blob/master/lstm_text_generation1111.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2SucB1tLWsL"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import nltk\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "#接下来，我们把文本读入\n",
        "raw_text = ''\n",
        "for file in os.listdir(\"input\"):\n",
        "\tif file.endswith(\".txt\"):\n",
        "\t\traw_text+=open(\"input/\"+file,errors='ignore',encoding='utf-8').read()+'\\n\\n'   #读入多个文件\n",
        "\n",
        "raw_text = raw_text.lower()\n",
        "sentensor = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "sents = sentensor.tokenize(raw_text)  #fen\n",
        "corpus = []\n",
        "for sen in sents:\n",
        "\tcorpus.append(nltk.word_tokenize(sen))\n",
        "\n",
        "print(len(corpus))\n",
        "print(corpus[:3])\n",
        "\n",
        "w2v_model = Word2Vec(corpus,size=128, window = 5, min_count = 5, workers = 4)\n",
        "\n",
        "print(w2v_model['office'])\n",
        "f\n",
        "# 把源数据变成一个长长的x，好让LSTM学会predict下一个单词\n",
        "\n",
        "raw_input = [item for sublist in corpus for item in sublist]\n",
        "\n",
        "print(raw_input[:100])\n",
        "\n",
        "text_stream = []\n",
        "vocab = w2v_model.wv.vocab\n",
        "for word in raw_input:\n",
        "\tif word in vocab:\n",
        "\t\ttext_stream.append(word)\n",
        "len(text_stream)\n",
        "\n",
        "#这里我们的文本预测积就是，给出了前面的单次以后，下一个单词是谁？\n",
        "#比我，hello from the other ,给出side\n",
        "\n",
        "#构造训练测试集\n",
        "#x 是前置字母们 y 是后一个字母\n",
        "\n",
        "seq_length=10\n",
        "x=[]\n",
        "y=[]\n",
        "for i in range(0, len(text_stream)-seq_length):\n",
        "\tgiven = text_stream[i:i+seq_length]\n",
        "\tpredict = text_stream[i+seq_length]\n",
        "\tx.append(np.array([w2v_model[word] for word in given]))\n",
        "\ty.append(w2v_model[predict])\n",
        "\n",
        "print(x[10])\n",
        "print(y[10])\n",
        "\n",
        "print(len(x))\n",
        "print(len(y))\n",
        "\n",
        "print(len(x[12]))\n",
        "print(len(x[12][0]))\n",
        "print(len(y[12]))\n",
        "x=np.array(x)\n",
        "y=np.array(y)\n",
        "x = np.reshape(x,(-1,seq_length,128))\n",
        "y = np.reshape(y,(-1,128))\n",
        "\n",
        "# 接下来我们做两件事：\n",
        "#\n",
        "#    1我们已经有了一个input的数字表达（w2v），我们要把它变成LSTM需要的数组格式： [样本数，时间步伐，特征]\n",
        "#\n",
        "#    2第二，对于output，我们直接用128维的输出\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(256,dropout_W=0.2, dropout_U=0.2,input_shape=(seq_length,128)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(128,activation='sigmoid'))\n",
        "model.compile(loss='mse',optimizer='adam')\n",
        "\n",
        "#跑模型\n",
        "model.fit(x,y,epochs=50,batch_size=4096)\n",
        "\n",
        "#接下来写个程序，看看我们训练出来的LSTM的效果：\n",
        "\n",
        "def predict_next(input_array):\n",
        "\tinput_array=np.array(input_array)\n",
        "\tx = np.reshape(input_array,(-1,seq_length,128))\n",
        "\ty = model.predict(x)\n",
        "\treturn y\n",
        "\n",
        "def string_to_index(raw_input):\n",
        "\traw_input = raw_input.lower()\n",
        "\tinput_stream = nltk.word_tokenize(raw_input)\n",
        "\tres = []\n",
        "\tfor word in input_stream[(len(input_stream)-seq_length):]:\n",
        "\t\tres.append(w2v_model[word])\n",
        "\treturn res\n",
        "\n",
        "def y_to_word(y):\n",
        "\tword = w2v_model.most_similar(positive = y,topn=1)\n",
        "\treturn word\n",
        "\n",
        "#写一个大程序\n",
        "def generate_article(init,rounds=30):\n",
        "\tin_string = init.lower()\n",
        "\tfor i in range(rounds):\n",
        "\t\tn=y_to_word(predict_next(string_to_index(in_string)))\n",
        "\t\tin_string +=' '+n[0][0]\n",
        "\treturn in_string\n",
        "\n",
        "init = 'Language Models allow us to measure how likely a sentence is, which is an important for Machine'\n",
        "article = generate_article(init)\n",
        "print(article)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}